一些问题

1. 不同模型可使用的tools差异很大，直接影响了模型在复杂工作上的表现。尤其是模型本身而不是产品为目标（如Qwen、Deepseek）时。
2. 部分tools在产品（如chatGPT）中，由于其system prompt、finetune和模型的结合，表现会明显好于直接调用模型本身（如GPT-5）
3. 模型对于工具的调用方式受到很多因素影响，需要统一的评测方式。
4. 模型的表现评价结果不应该因为工作流的较早阶段的错误而被过分影响，否则有失平衡性和对模型的能力充分评估。
5. 受到第三方平台影响，部分tools不可用。官方API无法访问（付款问题和国家地区问题）。

因此，我们需要一个统一的评测框架，来评测不同模型在使用tools时的表现。

我目前的想法

1. 评测框架需要支持多种模型和多种工具的接入。
2. 评测框架需要提供统一化的MCP接口。
3. 评测框架需要分步骤评分，并且保证每个步骤的相对原子性
4. 评分的综合性需要同时考虑任务工作流的熔断特点，和模型在各个步骤的表现。
5. 评测框架需要支持多种评测方式，自动化评测。

评测框架的搭建

1. 评测框架的搭建需要考虑到模型和工具的多样性，设计一个灵活的接口。尤其考虑相对低能力的模型，其能力在某些方面可能未必很差。
2. 评测工具集合需要包括，搜索引擎等（暂不明确）
3. 评测采用面试官-考生的对话形式，并根据成功通过该步骤需要的尝试次数和提供的具体信息量来评分。从越模糊的数据、越少的尝试次数、越少的工具调用次数、越少的推理消耗token量中成功通过该步骤，得分越高。
4. 评测步骤之间需要有原子性，以对比不同模型在任务的不同阶段的具体表现能力。
5. 最终，是否可以添加一个人工制作的best performing model的结果作为对比，比如不同的模型处理不同的具体任务完成完整工作流。
6. 评测工作可以有多个模态，最好包括更复杂的问题（比如根据图片、视频等进行分析和回答，回答形式也可以多样化，如文本、代码、表格甚至图片视频等）
7. 评测结果需要有多种维度的分析，既包括整体的任务完成情况，也包括各个步骤的具体表现。

评测重心：

- MCP?
- **Multi-modal**?
- Complex task agent?
- etc.